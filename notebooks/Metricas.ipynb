{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar los dos modelos creados previamente primero nos centraremos en crear un conjunto de prueba siguiendo los siguientes pasos:\n",
    "\n",
    "1. Obtener un conjunto de oraciones variadas (C)\n",
    "    - Oraciones simples\n",
    "    - Oraciones complejas \n",
    "    - Oraciones con ruido\n",
    "\n",
    "2. TP - que recupere la query tal cual o lo más parecido\n",
    "    - La query (c in C) sí se encuentra en corpus\n",
    "    - La query se encuentra parcialmente\n",
    "       (tomamos oraciones del corpus y le metemos ruido)\n",
    "       (tendríamos que revisar que el corpus si tenga la query parcial\n",
    "       y luego revisar los resultados, si la busqueda da resultados\n",
    "       pero en corpus no hay entonces es un falso positivo)\n",
    "\n",
    "\n",
    "3. TN - oraciones donde ninguna palabra está dentro del corpus\n",
    "    - Si regresa resultados es un falso negativo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/metricas.jpg\" width=\"70%\" alt=\"Metricas\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #particiones\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "#Para el corpus\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import Data \n",
    "from math import inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el corpus original para seleccionar algunas oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': '0.13', 'dur': '3.77', 'text': 'tu papá tuvo un accidente'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_original = Data(\"../corpus/data\")\n",
    "videos_original.corpus[0][0]['subtitles'][0]#Nota: no print de todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el conjunto de test\n",
    "_, test = train_test_split(videos_original.corpus, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtitles():\n",
    "    '''\n",
    "    Regresa los subtitulos del corpus\n",
    "    '''\n",
    "    test_subtitles = []\n",
    "    for chanel in test:\n",
    "        for video in chanel:\n",
    "            if 'subtitles' in video:\n",
    "                for s in video['subtitles']:\n",
    "                    test_subtitles.append(s['text'])\n",
    "    return test_subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del .5% de oraciones tomadas del corpus tomamos la mitad para meterles ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estable el rojo de doble hoja escuchen', 'me vea distinto voy a presionar para', 'nos vamos a conectar realmente está', 'sobrepasando los límites tomando riesgos', '[Música]', 'tu espíritu es evidente', 'tiene tanta imaginación como miedo de', 'tal vez ninguno de ellos y te querrá', 'tuve que traer a mi hermano a vivir', '[Música]'] 1814 1814\n"
     ]
    }
   ],
   "source": [
    "test_TP, test_TPR = train_test_split(get_subtitles(), test_size=0.5)\n",
    "print(test_TPR[:10], len(test_TPR), len(test_TP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['estable el Krpvai rojo de doble hoja Yiq09t escuchen', 'me vea distinto YM9WvL voy a WmXDRl presionar para', 'nos vamos FvWJCa CuwjNU a conectar realmente está', 'WcKt0L sobrepasando los límites sQ7Bre tomando riesgos', 'lcIm6W tunOy4 [Música]', 'Q7qGDB tu 328igU espíritu es evidente', 'tiene k93N1s tanta h52ltC imaginación como miedo de', 'tal vez ninguno voKmKN de CqymG2 ellos y te querrá', 'tuve que 5by4Qg traer kNRISK a mi hermano a vivir', 'PpbZIn i41K3Y [Música]'] 1814\n"
     ]
    }
   ],
   "source": [
    "def insert(s, num):\n",
    "    \"\"\"\n",
    "    Agrega n cadenas aleatorias a una lista\n",
    "\n",
    "    Args:\n",
    "        s (list): lista de cadenas\n",
    "        num (int): número de cadenas aleatorias a insertar\n",
    "    \"\"\"\n",
    "    for _ in range(num):\n",
    "        noise = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(6))\n",
    "        ix = random.randint(0,len(s)-1)\n",
    "        s.insert(ix, noise)\n",
    "\n",
    "def make_noise():\n",
    "    \"\"\"\n",
    "    A cada oración del conjunto le agrega n cadenas aleatorias\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(test_TPR):\n",
    "        s = sentence.split(\" \")\n",
    "        insert(s, 2)\n",
    "        test_TPR[i] = ' '.join(s)\n",
    "\n",
    "make_noise()\n",
    "print(test_TPR[:10], len(test_TPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos .12% de oraciones con palabras que no están en el corpus de tal manera que el conjunto de prueba queda compuesto por:\n",
    "- 0.25% oraciones que se encuentran tal cual en el corpus\n",
    "- 0.25% oraciones que se encuentran  en el corpus pero con ruido\n",
    "- 0.12% oraciones que no comparten palabras con el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aDn0iN 2yFbJ1 iYySec UIV2nH WvbwKm ', 'DIm6hY lwDrf1 mAGUp5 ', 'zLPNEn e2KMX2 lWmPpC RwpiHX 0mNqTY j60FLh CMoPON p7aifF bWuCLr 9eQXHu ', 'HBu9cb DnDiVN DJUQTT s9DINu mEeRUJ 4FEUKw fmPMtA x2nrcC gOsFXC P8W64E ', 'KyEcMj I9iWB1 jc6Td1 ', 'Y0xPOl VjFXvb L26OJ7 KvKiYO Or9KJB Esz882 8PrcRL DNk53s HMfqzN FpueVf 2tnASF 2lR8cI 8Ls1w4 P3wUkM zbz3Qe SlNbra eQY1NG H7QlCe fLCJc9 VVpYVg ', 'E3Xyux yhqYwK 24tzkl ', 'PNjtxM AjP2HI liE5zy ', 'gAyhM4 7o64aj 5ONjIw G4q3GD NbU0k9 ', 'd4Uzpx KN4izO lfKZpu '] 907\n"
     ]
    }
   ],
   "source": [
    "def make_sentence():\n",
    "    \"\"\"\n",
    "    Crea un conjunto de cadenas aleatorias\n",
    "    \"\"\"\n",
    "    sentence = []\n",
    "    lg = [3,5,10,20]\n",
    "    for _ in range(len(test_TP)//2):\n",
    "        s = [\"\"]\n",
    "        insert(s, lg[random.randint(0,len(lg)-1)])\n",
    "        sentence.append(' '.join(s))\n",
    "    return sentence\n",
    "    \n",
    "test_TN = make_sentence()\n",
    "print(test_TN[:10], len(test_TN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = test_TP + test_TPR + test_TN\n",
    "pickle.dump(test_all, open(\"../pkl/test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4535"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../pkl/test.pkl\", \"rb\") as f:\n",
    "    test_all = pickle.load(f)\n",
    "\n",
    "len(test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4535"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veremos que tan bien recupera las oraciones de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(s1, s2):\n",
    "    \"\"\"\n",
    "    Regresa el número de palabras que conparten 2 cadenas\n",
    "\n",
    "    Args:\n",
    "        s1 (lst): lista de palabras de la cadena 1\n",
    "        s2 (lst): lista de palabras de la cadena 2\n",
    "    \"\"\"\n",
    "    val = 0\n",
    "    for i in s2: \n",
    "        if i in s1: val += 1\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words(\"a un lado oigan alejen a los demás viene\".split(), 'y fue porque íbamos a saltar de los'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el caso de TFIDF cuando no encuentra coincidencias\n",
    "#(score = 0 para todas las cadenas)regresa lo siguiente:\n",
    "dummy_sentence = ['tu papá tuvo un accidente', 'te recuerdo que está', \n",
    "                  'aquí', 'estaba como rabioso con', \n",
    "                  'especial con lucas martín']\n",
    "\n",
    "def eval_sentence(query, resp):\n",
    "    \"\"\"\n",
    "    Evalúa los resultados obtenidos dada una query\n",
    "\n",
    "    Args:\n",
    "        query (str): query de prueba generada previamente\n",
    "        resp (list): lista con las primeras 5 respuestas obtenidas \n",
    "            por algún mecanismo de RI\n",
    "    Return:\n",
    "        (int): 0 cuando no encontró ninguna coincidencia\n",
    "               1 si encontró la query exacta y es el primer resultado\n",
    "               (0 - 1) si:\n",
    "                - Encuentra la query pero no es el primer resultado\n",
    "                le quitamos .1 por cada lugar que pasa y no está la query\n",
    "                - No se encuentra la query pero algunas palabras de la misma sí \n",
    "    \"\"\"\n",
    "    val = 0 \n",
    "    total = 0\n",
    "\n",
    "    if resp != dummy_sentence: \n",
    "        count = 0\n",
    "        for i, r in enumerate(resp):\n",
    "            if r != \"\":\n",
    "                if r == query: #si la encuentra val será como min .6\n",
    "                    return  1 - (.1 * i)\n",
    "                total += len(r)\n",
    "                count += count_words(query.split(), r.split())\n",
    "                # si no la encuentra ve que tanto se parecen los resultados\n",
    "                val = count / total \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos los 4 casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(\"avergonzado lo suficiente no apenas\",\n",
    "             ['avergonzado lo suficiente no apenas', \n",
    "             'pesados o polar no lo suficiente como', \n",
    "             'no vende suficiente leña lo siento pero', \n",
    "             'Como pueden ver, no me cuesta trabajo \\nestirar la masa porque descanso lo suficiente.', \n",
    "             'aún así no es suficiente porque lo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(\"avergonzado lo suficiente no apenas\",\n",
    "             ['pesados o polar no lo suficiente como', \n",
    "             'no vende suficiente leña lo siento pero', \n",
    "             'avergonzado lo suficiente no apenas', \n",
    "             'Como pueden ver, no me cuesta trabajo \\nestirar la masa porque descanso lo suficiente.', \n",
    "             'aún así no es suficiente porque lo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056910569105691054"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(\"a un lado oigan alejen a los demás viene\",   \n",
    "             ['Vámonos a casa.', \n",
    "             '¿Cómo ibas a saberlo?', \n",
    "             '¡Hola a todos!', \n",
    "             'Son fisuras\\na un infierno desconocido.', \n",
    "             'y fue porque íbamos a saltar de los'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_sentence(\"8eIqyc xwWsUF 3eHwS8 qE5XFM Tknsyj\",\n",
    "             ['tu papá tuvo un accidente', \n",
    "             'te recuerdo que está', \n",
    "             'aquí', \n",
    "             'estaba como rabioso con', \n",
    "             'especial con lucas martín'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../pkl/test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all(resp):\n",
    "    \"\"\"\n",
    "    Evalua las respuestas de un modelo de la siguiente manera:\n",
    "        Si está en el conjunto 1 (original) debería de responder con 1\n",
    "            Si 1 o muy cerca de 1 TP++ else si 0 FN++\n",
    "        Si esta en el conjunto 2 (con ruido) debería responder con un número decimal\n",
    "            Si mayor a .5 TP++ else si menor a 5 FN++\n",
    "        Si está en el conjunto 3 (que no están) deberia responder con 0\n",
    "            Si 0 TN++ else si 1 FP++\n",
    "    \n",
    "    Args:\n",
    "        resp (list): cada entrada de la lista es una lista con 5 respuestas\n",
    "    \"\"\"\n",
    "    original_noise = zip(test[:3628], resp[:3628])\n",
    "    invent = zip(test[3629:-1], resp[3629:-1])\n",
    "    TP, FN, FP, TN = 0, 0, 0, 0\n",
    "    #Recuperamos las particiones\n",
    "    #Conjunto original y con ruido\n",
    "    for query, resp in original_noise:\n",
    "        r = eval_sentence(query, resp)\n",
    "        if r > 0.4:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN +=1\n",
    "    #Conjunto inventado\n",
    "    for query, resp in invent:\n",
    "        r = eval_sentence(query, resp)\n",
    "        if r < 0.5:\n",
    "            TN += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    return TP, FN, FP, TN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_micro(resp, take=10): #30\n",
    "    \"\"\"\n",
    "    Evalua las respuestas de un modelo de la siguiente manera:\n",
    "        Si está en el conjunto 1 (original) debería de responder con 1\n",
    "            Si 1 o muy cerca de 1 TP++ else si 0 FN++\n",
    "        Si esta en el conjunto 2 (con ruido) debería responder con un número decimal\n",
    "            Si mayor a .5 TP++ else si menor a 5 FN++\n",
    "        Si está en el conjunto 3 (que no están) deberia responder con 0\n",
    "            Si 0 TN++ else si 1 FP++\n",
    "    Toma solo los primeros \"take\" elementos para hacer los tests\n",
    "    Args:\n",
    "        resp (list): cada entrada de la lista es una lista con 5 respuestas\n",
    "    \"\"\"\n",
    "    micro_test = test[:take]+test[1814:1814+take]+test[1814*2:1814*2+take]\n",
    "    original_noise = zip(micro_test[:take*2], resp[:take*2])\n",
    "    invent = zip(test[take*2:-1], resp[take*2:-1])\n",
    "    TP, FN, FP, TN = 0, 0, 0, 0\n",
    "    #Recuperamos las particiones\n",
    "    #Conjunto original y con ruido\n",
    "    for query, resp in original_noise:\n",
    "        r = eval_sentence(query, resp)\n",
    "        if r > 0.4:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN +=1\n",
    "    #Conjunto inventado\n",
    "    for query, resp in invent:\n",
    "        r = eval_sentence(query, resp)\n",
    "        if r < 0.5:\n",
    "            TN += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    return TP, FN, FP, TN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(TP, FN, FP, TN):\n",
    "    \"\"\"\n",
    "    Realiza el cálculo de:\n",
    "    Sensitive, specificity, precision, \n",
    "    negative predictive value y accuracy \n",
    "\n",
    "    Args:\n",
    "        TP (int): True Positive\n",
    "        FN (int): False Negative\n",
    "        FP (int): False Positive\n",
    "        TN (int): True Negative\n",
    "    \"\"\"\n",
    "    pred = TP / (TP + FP)  if (TP + FP) != 0 else inf\n",
    "    rec =  TP / (TP + FN)  if (TP + FN) != 0 else inf\n",
    "    return {\"TP\": TP, \"FN\": FN, \"FP\": FP, \"TN\": TN,\n",
    "            \"Specificity\": TN / (TN + FP)  if (TN + FP) != 0 else inf,\n",
    "            \"Negative predictive value\": TN / (TN + FN) if (TN + FN) != 0 else inf ,\n",
    "            \"Precision\": pred,\n",
    "            \"Sensitive (recall)\": rec,\n",
    "            \"Accuracy\": (TP + TN) / (TP + TN + FP + FN),\n",
    "            \"F1\": 2*(pred * rec) / (pred + rec)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos TF-IDF con videos por vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 529,\n",
       " 'FN': 3099,\n",
       " 'FP': 0,\n",
       " 'TN': 905,\n",
       " 'Specificity': 1.0,\n",
       " 'Negative predictive value': 0.22602397602397603,\n",
       " 'Precision': 1.0,\n",
       " 'Sensitive (recall)': 0.1458103638368247,\n",
       " 'Accuracy': 0.31634679020516215,\n",
       " 'F1': 0.2545104642771229}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../pkl/results.pkl\", \"rb\") as f:\n",
    "    test_results = pickle.load(f)\n",
    "\n",
    "TP, FN, FP, TN = eval_all(test_results)\n",
    "get_metrics(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos TF-IDF con videos por subtítulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 1093,\n",
       " 'FN': 2535,\n",
       " 'FP': 0,\n",
       " 'TN': 905,\n",
       " 'Specificity': 1.0,\n",
       " 'Negative predictive value': 0.26308139534883723,\n",
       " 'Precision': 1.0,\n",
       " 'Sensitive (recall)': 0.3012679162072767,\n",
       " 'Accuracy': 0.44076770350761085,\n",
       " 'F1': 0.46303749205676764}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../pkl/resultsxOracion.pkl\", \"rb\") as f:\n",
    "    test_resultsxO = pickle.load(f)\n",
    "\n",
    "TP, FN, FP, TN = eval_all(test_resultsxO)\n",
    "get_metrics(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 1448,\n",
       " 'FN': 2180,\n",
       " 'FP': 0,\n",
       " 'TN': 905,\n",
       " 'Specificity': 1.0,\n",
       " 'Negative predictive value': 0.293354943273906,\n",
       " 'Precision': 1.0,\n",
       " 'Sensitive (recall)': 0.39911797133406834,\n",
       " 'Accuracy': 0.5190822854621663,\n",
       " 'F1': 0.570527974783294}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../pkl/resultsDoc2Vec.pkl\", \"rb\") as f:\n",
    "    test_resultsD2V = pickle.load(f)\n",
    "\n",
    "TP, FN, FP, TN = eval_all(test_resultsD2V)\n",
    "get_metrics(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 0,\n",
       " 'FN': 18,\n",
       " 'FP': 0,\n",
       " 'TN': 8,\n",
       " 'Specificity': 1.0,\n",
       " 'Negative predictive value': 0.3076923076923077,\n",
       " 'Precision': inf,\n",
       " 'Sensitive (recall)': 0.0,\n",
       " 'Accuracy': 0.3076923076923077,\n",
       " 'F1': nan}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../pkl/faux_levenshtein.pkl\", \"rb\") as f:\n",
    "    test_resultsFL = pickle.load(f)\n",
    "test_resultsFL = [[\" \".join(words) for words in sentence] for sentence in test_resultsFL]\n",
    "TP, FN, FP, TN = eval_micro(test_resultsFL, 9)\n",
    "get_metrics(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
